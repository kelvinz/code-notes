


<https://www.udemy.com/machine-learning-with-javascript/>



# what is machine learning



## problem solving process

1.	identify the independent & dependent variables
	*features* - are categories of data points that affect the value of a label

2.	assemble a set of data related to the problem you're trying to solve
	datasets almost always require cleanup or formatting

3.	decide on the type of output you are predicting
	*classification* - labels belong to a discret set
	*regression* - labels belong to a continuous set

4.	based on type of output,
	pick an algorithm that will determine a correlation between your 'feature' & 'labels'
	many algorithms exist, each with pros & cons

5.	use model generated by algorithm to make a prediction
	models relate the value of 'features' to the value of 'labels'



---



# algorithm overview



## k-nearest neighbor ( knn )

one of the simplest machine learning algorithms
essentially averaging the most similar inputs previously & their results

*eg. which bucket will a ball go into if dropped at 300px*

-	drop a ball a bunch of times all around the board
-	for each observation, subtract drop point from 300px to get absolute value
-	sort the results from least to most
-	look at the 'k' top records, what was the most frequent bucket
-	the more frequent bucket is our prediction

*note: k is a generic number which can be experimented & adjusted to suit different tasks*



## lodash

javascript utility library

```js

	const numbers = [
		[ 10, 5 ],
		[ 17, 2 ],
		[ 34, 1 ]
	]

	const sorted = _.sortBy( numbers, row => row[ 1 ] )

	//	results in
	sorted = [
		[ 34, 1 ],
		[ 17, 2 ],
		[ 10, 5 ]
	]

	const mapped = _.map( sorted, row => row[ 1 ] )

	//	results in
	mapped = [ 1, 2, 5 ]

	//	chaining
	//	_.chain to start chain
	//	.value() to get result
	const chained = _.chain( numbers )
						.sortBy( row => row[ 1 ] )
						.map( row => row[ 1 ] )
						.value();

```



## implementing knn

```js

	const outputs = [
		//	drop position, bounciness, ball size, result
		[ 10, .5, 16, 1 ],
		[ 200, .5, 16, 4 ],
		[ 350, .5, 16, 4 ],
		[ 600, .5, 16, 5 ]
	]

	const
	predictionPoint = 300,
	k = 3

	function distance( point, predictionPoint ) {
		return Math.abs( point - predictionPoint )
	}

	//	using lodash
	_.chain( outputs )
		//	get abs difference & results into an array, ignoring bounce & size
		.map( row => [ distance( row[ 0 ], predictionPoint ), row[ 3 ] ] )
		//	sort difference from lowest to highest
		.sortBy( row => row[ 0 ] )
		//	get top k records
		.slice( 0, k )
		//	get object with value of row[ 1 ] & number of times it occurs
		//	countBy changes value/keys into strings
		//	note to change back to num later
		.countBy( row => row[ 1 ] )
		//	take object & convert to array of arrays
		.toPairs( )
		//	sort most commonly occurring bucket from low to high
		.sortBy( row => row[ 1 ] )
		//	return last element aka [ "4", "2" ]
		.last()
		//	return first element aka "4"
		.first()
		//	change to num
		.parseInt()
		.value();

```



## training & test data

split all data into training & test

*training&* is used to implement the knn formula, aka train our algo
*test* use the trained algo & check how accurate our algo is

note: always shuffle data before splitting to ensure no patterns exists in our data
eg. our data could come in from small to large or arranged in some particular order we don't know about

```js

	function splitDataset( data, testCount ) {
		const shuffled = _.shuffle( data )

		const testSet = _.slice( shuffled, 0, testCount )
		const trainingSet = _.slice( shuffled, testCount )

		return [ testSet, trainingSet ]
	}

	//	modified from above to be generalized
	function knn( data, point ) {
		return _.chain( data )
			.map( row => [ distance( row[ 0 ], point ), row[ 3 ] ] )
			.sortBy( row => row[ 0 ] )
			.slice( 0, k )
			.countBy( row => row[ 1 ] )
			.toPairs()
			.sortBy( row => row[ 1 ] )
			.last()
			.first()
			.parseInt()
			.value()
	}

	function runAnalysis() {
		const testSetSize = 10
		const [ testSet, trainingSet ] = splitDataset( outputs, testSetSize )

		let numberCorrect = 0

		for ( let i = 0; i < testSet.length; i++ ) {
			const bucket = knn( trainingSet, testSet[ i ][ 0 ] )

			if ( bucket === test[ i ][ 3 ] ) {
				numberCorrect++
			}
		}

		console.log( 'Accuracy', numberCorrect / testSetSize )
	}

```



## refactoring accuracy report

```js

	//	refactored from top
	function runAnalysis() {
		const testSetSize = 10
		const [ testSet, trainingSet ] = splitDataset( outputs, testSetSize )

		const accuracy = _.chain( testSet )
			.fliter( testPoint => knn( trainingSet, testPoint[ 0 ] ) === testPoint[ 3 ] )
			.size()
			.divide( testSetSize )
			.value()

		console.log( 'Accuracy is ', accuracy )
	}

```



## investigating optimal k values

```js

	function runAnalysis() {
		const testSetSize = 50
		const [ testSet, trainingSet ] = splitDataset( outputs, testSetSize )

		_.range( 1, 15 ).forEach( k => {
			const accuracy = _.chain( testSet )
				.fliter( testPoint => knn( trainingSet, testPoint[ 0 ], k ) === testPoint[ 3 ] )
				.size()
				.divide( testSetSize )
				.value()

			console.log( 'For k of ', k, ' accuracy is ', accuracy )
		}
	}

```



## multi-dimensional knn

updating the distance function to use the pythagorean theorem

`c ** 2 = a ** 2 + b ** 2`
`c = ( a ** 2 + b ** 2 ) ** .5`

with more than 2 features, we just add more x ** 2 into equation

```js

	function distance( pointA, pointB ) {

		//	where pointA is an array
		_.chain( pointA )
			//	zip takes arrays
			//	makes each pair at point into its own array
			.zip( pointB )
			.map( ( [ a, b ] ) => ( a - b ) ** 2 )
			.sum()
			.value() ** .5
	}

	//	returns array without the last data
	//	without modifying actual array
	_.initial( row )

	//	returns last data of array
	//	without modifying actual array
	_.last( row);

```



## magnitude offsets in features

because the range of different features may be very different
the effects of it on the calculation might become offscaled
thus we need to normalize our data or standardize them

-	*normalize*
	make data range between 0 to 1
	data will only be between 0 to 1

-	*standardize*
	make majority of data near 0, from -1 to 1
	possible for outliers to exceed -1 or 1

```js

	//	get min & max value
	//	normalize points to minmax

	const points = [ 200, 150, 650, 430 ]

	const min = _.min( points )
	const max = _.max( points )

	_.map( points, point => {
		return ( point - min ) / ( max - min )
	} );

```



## normalization with minmax

```js

	//	feature count will let us know
	//	that the last data in array is its result
	//	not a feature
	function minMax( data, featureCount ) {
		//	clone so we don't modify actual data
		const clonedData = _.cloneDeep( data )

		//	i is column in clonedData array
		//	getting each feature we want to normalize
		for ( let i = 0; i < featureCount; i++ ) {
			const column = clonedData.map( row => row[ i ] )

			const min = _.min( column )
			const max = _.max( column )

			//	j is row in clonedData array
			//	getting each set of data
			for ( let j = 0; j <  clonedData.length; j++ ) {
				//	modify array
				clonedData[ j ][ i ] = ( clonedData[ j ][ i ] - min ) / ( max - min )
			}
		}

		return clonedData
	}

```



---



# onwards to tensorflow js



## tensor shape & dimension

**tensor**
something like an array with extra functionalities

**dimensions**
1d = array
2d = array of array
3d... 4d...
one trick is to count the number of opening brackets in succession
[ [ 1, 2 ], [ 3, 4 ] ] = 2d

**shape**
imagine calling .length once on each dimention from outside in

[ 5, 10, 17 ]
*shape* = [ 3 ]

[
	[ 5, 10, 17 ],
	[ 18, 4, 2 ]
]
*shape* = [ 2, 3 ]
something like [ rows, columns ]

[
	[
		[ 5, 10, 17 ]
	]
]
*shape* = [ 1, 1, 3 ]



## elementwise operations

```js

	const data = tf.tensor( [ 1, 2, 3 ] )
	const otherData = tf.tensor( [ 4, 5, 6 ] )

	data.shape	//	[ 3 ]

	//	elementwise operations return a new tensor
	//	does not affect original tensors

	data.add( otherData )	//	[ 5, 7, 9 ]
	data.sub( otherData )	//	[ -3, -3, -3 ]
	data.mul( otherData )	//	[ 4, 10, 18 ]
	data.div( otherData );	//	[ .25, .4, .5 ]

	//	if the shapes don't match
	//	we can't do elementwise operations

```



## broadcasting operations

```js

	const data = tf.tensor( [ 1, 2, 3 ] )
	const otherData = tf.tensor( [ 4 ] )

	//	if the shapes don't match
	//	there are edge cases where we can still do operations
	//	this is known as broadcasting

	//	only works if
	//	after stacking shape up
	//	comparing columns from right to left
	//	shapes are equal or one is '1'

	[ 1, 2, 3 ] + [ 4 ]
	shape = [ 3 ] + shape = [ 1 ]
	//	stack the shapes up
	[ 3 ]
	[ 1 ]
	//	one of it is 1

	[ [ 1, 2, 3 ], [ 4, 5, 6 ] ] + [ [ 1 ], [ 1 ] ]
	shape = [ 2, 3 ] + shape = [ 2, 1 ]
	//	stack the shapes up
	[ 2, 3 ]
	[ 2, 1 ]
	//	first col has a 1
	//	second col both are 2 so its equal

	//	kinda means smearing the [ [ 1 ], [ 1 ] ] over the other array

	shape = [ 2, 3, 2 ] + shape = [ 3, 1 ]
	//	stack the shapes up
	[ 2, 3, 2 ]
	   [ 3, 1 ]
	//	2, 1 has a 1
	//	3, 3 is the same
	//	last one has no value on bottom col so we skip

	shape = [ 2, 3, 2 ] + shape = [ 2, 1 ]
	//	stack the shapes up
	[ 2, 3, 2 ]
	   [ 2, 1 ]
	//	2, 1 has a 1
	//	3, 2 doesn't match
	//	thus this operation cannot be done

```



## misc

```js

	//	logging tensor data
	const data = tf.tensor( [ 1, 2, 3 ] )
	data.print()
	//	console log out tensor
	//	results => tensor [ 1, 2, 3 ]



	//	tensor accessors
	data.get( 0 )

	const data = tf.tensor( [ [ 1, 2, 3 ], [ 40, 50, 60 ] ] )
	data.get( 0, 0 );
	//	row, col
	//	results => 1

	//	no set method
	//	you can't modify values in tensors
	//	have to create new tensor



	//	slice data
	const data = tf.tensor( [ [ 1, 2, 3 ], [ 40, 50, 60 ] ] )
	//	start index, size
	//	index starts with 0
	//	size starts with 1
	//	size => rows, cols
	data.slice( [ 0, 1 ], [ 2, 1 ] )
	//	results in [ [ 1 ], [ 40 ] ]
	//	or use -1 for 'all' from point to end
	data.slice( [ 0, 1 ], [ -1, 1 ] );



	//	joining data
	const tensorA = tf.tensor([
		[ 1, 2, 3 ],
		[ 4, 5, 6 ]
	])

	const tensorB = tf.tensor([
		[ 7, 8, 9 ],
		[ 10, 11, 12 ]
	])

	tensorA.concat( tensorB, 0 )
	//	imagine arrow going top down
	//	results => [ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ], [ 10, 11, 12 ] ]

	tensorA.concat( tensorB, 1 )
	//	imagine arrow going left to right
	//	results => [ [ 1, 2, 3, 7, 8, 9 ], [ 4, 5, 6, 10, 11, 12 ] ]



	//	summing along axis
	const jumpData = tf.tensor([
		[ 70, 70, 70 ],
		[ 70, 70, 70 ]
	])

	const playerData = tf.tensor([
		[ 1, 160 ],
		[ 2, 160 ]
	])
	//	if no second argument given in sum
	//	it will sum all numbers into one single result
	//	second arguement if added is the axis to go by

	jumpData.sum( 0 )
	//	imagine arrow going top down
	//	results => [ 140, 140, 140 ]

	jumpData.sum( 1 )
	//	imagine arrow going left to right
	//	results => [ 210, 210 ]



	//	massaging dimensions
	jumpData.sum( 1 ).concat( playerData, 1 )
	//	error
	//	sum reduces dimensions
	//	[ 2, 3 ] => [ 2 ]

	//	fix 1
	//	keep dimension when summing
	jumpData.sum( 1, true )

	//	fix 2
	//	more robust, useful in different situations
	//	expandDims increase dimensions
	//	expand in which axis, row - 0, col - 1
	jumpData.sum( 1 ).expandDims( 1 );

```



---



# applications of tensorflow



## knn

using regression instead of classification like example above

```js

	//	location of house predict price

	//	long/lat of house
	const features = tf.tensor([
		[ -121, 47 ],
		[ -121.2, 46.5 ],
		[ -122, 46.4 ],
		[ -120.9, 46.7 ]
	])

	//	house price
	const labels = tf.tensor([
		[ 200 ],
		[ 250 ],
		[ 215 ],
		[ 240 ]
	])

	//	long/lat to predict price for
	const predictionPoint = tf.tensor( [ -121, 47 ] )

	const k = 2

	features
		//	find difference between prediction point vs actual points
		//	subtract prediction point on every feature point
		.sub( predictionPoint )
		//	square each of the difference
		.pow( 2 )
		//	add all rows up
		.sum( 1 )
		//	square root each row
		.pow( .5 )
		//	up dim
		.expandDims( 1 )
		//	join with labels
		.concat( labels, 1 )
		//	unstack make each row a new tensor
		//	add them into an array
		.unstack()
		//	sort from a - z or 0 - 10 by default
		//	function with a, b
		//	return 1 will be below, -1 will be on top
		.sort( ( a, b ) => {
			return a.get( 0 ) > b.get( 0 ) ? 1 : -1
		} )
		//	get top k records
		//	this is an array slice not tf slice
		//	after unstack we are working with arrays returned
		.slice( 0, k )
		//	sum the labels
		//	divide by k to get average
		.reduce( ( acc, obj ) => {
			return acc + obj.get( 1 )
		}, 0 ) / k

```



## standardization

( value - average ) / standard deviation

```js

	const numbers = tf.tensor([
		[ 1, 2 ],
		[ 3, 4 ],
		[ 5, 6 ]
	])

	//	mean = average
	//	sqrt( variance ) = standard deviation
	//	if no 2nd value, moments combines all values
	//	2nd value is axis if needed
	const { mean, variance } = tf.moments( numbers, 0 )

	numbers.sub( mean ).div( variance.pow( .5 ) );

```



## final knn

```js

	function knn( features, labels, predictionPoint, k ) {

		const { mean, variance } = tf.moments( features, 0 )
		const scaledPrediction = scaledPrediction.sub( mean ).div( variance.pow( .5 ) )

		return (
			features
				.sub( mean )
				.div( variance.pow( .5 ) )
				.sub( scaledPrediction )
				.pow( 2 )
				.sum( 1 )
				.pow( .5 )
				.expandDims( 1 )
				.concat( labels, 1 )
				.unstack()
				.sort( ( a, b ) => ( a.get( 0 ) > b.get( 0 ) ? 1 : -1 ) )
				.slice( 0, k )
				.reduce( ( acc, pair ) => acc + pair.get( 1 ), 0 ) / k
		)
	}

```



---



# getting started with gradient descent



## linear regression

drawing a trendline on a chart of points
results in a equation that can be used to do predictions

many ways to solve linear regression
gradient descent is one of them



## calculate how far guess is from actual

-	**mean squared error ( mse )**
	loop all with i
	add all of ( guess[ i ] - actual[ i ] ) squared
	divide by number of i



## derivatives

slope of mse can be used to arrive at optimal guess
derivative can be used to get the slope

*negative number means slopping to the right*



## gradient descent in action

line equation is
`y = mx + b`

1.	pick a value for 'b' & 'm'
2.	calculate the slope of mse with respect to 'b' & 'm'
3.	check if both slopes are very small, if so, end
4.	multiply slopes by arbitrary value - 'learning rate'
5.	subtract results from 'b' & 'm'
6.	repeat step 2 till end



**to be updated with more details on the math**



---



# gradient descent with tensorflow



## basic class for learning formula

```js

	class LinearRegression {

		constructor( features, labels, options ) {
			this.features = features
			this.labels   = labels

			//	object assign allows you to set defaults
			//	which will be overriden if options are passed in
			//	assigning options into an obj
			//	this obj could be an empty obj {}
			//	or with the defaults inside like below
			this.options  = Object.assign( {
				learningRate: .1,
				iterations: 1000
			}, options )

			//	inital guesses
			this.m = 0
			this.b = 0
		}

		gradientDescent() {
			const currentGuesses = this.features.map( row => {
				//	mx + b
				return this.m * row[ 0 ] + this.b
			} )

			//	add all guess - actual
			const bSlope = _.sum( currentGuesses.map( ( guess, i ) => {
				//	guess - actual
				return guess - this.labels[ i ][ 0 ]
			//	multiply results by 2
			//	then divide total by length of array
			} ) ) * 2 / this.features.length

			const mSlope = _.sum( currentGuesses.map( ( guess, i ) => {
				//	-x * ( actual - guess )
				return -1 * this.features[ i ][ 0 ] * ( this.labels[ i ][ 0 ] - guess )
			//	multiply results by 2
			//	then divide total by length of array
			} ) ) * 2 / this.features.length

			this.m = this.m - mSlope * this.options.learningRate
			this.b = this.b - bSlope * this.options.learningRate
		}

		train() {
			for ( let i = 0; i < this.options.iterations; i++ ) {
				this.gradientDescent()
			}
		}
	}

```



## matrix multiplication



### are two matrices eligible to be multiplied together

put two shapes of two tensors ( matrices ) together side by side
are the two numbers next to each other the same
if they are the same, they can be multiplied

*eg.*
shapeA = [ 4, 2 ]
shapeB = [ 2, 3 ]
toegther => [ 4, 2 ] [ 2, 3 ]
since 2, 2 are the same, they can be multiplied

in matrices
A * B is not the same as B * A



### what's the output of matrix multiplication

remove the inner shapes from above
[ 4, 2 ][ 2, 3 ]
results of multiplication is a [ 4, 3 ] shaped matrix



### how is matrix multiplication done

... skip



## tensorflow version of above

slope of mse with respect to m & b toegther
( features * ( ( features * weights ) - labels ) ) / n

*where*
labels  = tensor of our label data
features = tensor of our features data
n       = number of observations
weights = m & b in a tensor



---



# increasing performance with vectorized solutions



## tensorflow linear regression

```js

	class LinearRegression {
		constructor( features, labels, options ) {
			this.features = tf.tensor( features )
			this.labels = tf.tensor( labels )

			//	tf.ones create tensor with all 1s based on shape given
			this.features = tf.ones( [ this.features.shape[ 0 ], 1 ] )
								.concat( this.features, 1 )

			this.options = Object.assign( {
				learningRate: .1,
				iterations: 1000
			}, options )
		}

		//	creating m & b into a tensor [ 0, 0 ]
		this.weights = tf.zeros( [ 2, 1 ] )

		gradientDescent() {
			//	matMul is matrix multiplication
			const currentGuesses = this.features.matMul( this.weights )
			const differences = currentGuesses.sub( this.labels )

			const slopes = this.features
								.transpose() //	switch up the shape 1, 2 to 2, 1, etc
								.matMul( differences )
								.div( this.features.shape[ 0 ] )

			//	new weights = weights subtracted from slope multiplied by learning rate
			this.weights = this.weights.sub( slopes.mul( this.options.learningRate ) )
		}

		train() {
			for ( let i = 0; i < this.options.iterations; i++ ) {
				this.gradientDescent()
			}
		}
	}

```



## calculating model accuracy

```js

	class LinearRegression {
		...

		//	testing results
		test( testFeatures, testLabels ) {
			testFeatures = tf.tensor( testFeatures )
			testLabels = tf.tensor( testLabels )

			testFeatures = tf.ones( [ testFeatures.shape[ 0 ], 1 ] )
								.concat( testFeatures, 1 )

			const predictions = testFeatures.matMul( this.weights )

			//	coefficient of determination
			const res = testLabels
						.sub( predictions )
						.pow( 2 )
						.sum()
						.get()

			const tot = testLabels
						.sub( testLabels.mean() )
						.pow( 2 )
						.sum()
						.get()

			return 1 - res / tot
			//	if results is negative
			//	taking average is more accurate than your slope
		}
	}

```



## standardization

```js

	class LinearRegression {

		//	helper function for constructor & test
		//	since there are repeated scripts
		processFeatures( features ) {
			features = tf.tensor( features )

			if ( this.mean && this.variance ) {
				//	if mean & variance already exists
				//	use them & not re-generate new one
				features = features.sub( this.mean ).div( this.variance.pow( .5 ) )
			} else {
				//	generate new mean & variance
				//	do standarization
				features = this.standardize( features )
			}

			features = tf.ones( [ features.shape[ 0 ], 1 ] )
							.concat( features, 1 )

			return features
		}

		standardize( features ) {
			const { mean, variance } = tf.moments( features, 0 )

			this.mean = mean
			this.variance = variance

			return features.sub( mean ).div( variance.pow( .5 ) )
		}

	}

```



## multivariate regression

```js

	class LinearRegression {
		...

		constructor( features, labels, options ) {
			...

			this.weights = tf.zeros( [ this.features.shape[ 1 ], 1 ] )
		}
	}

```



## learning rate optimization

eg. adam, adagrad, rmsprop, momentum

```js

	class LinearRegression {
		constructor( features, labels, options ) {
			...
			this.mseHistory = []
		}

		...

		train() {
			for ( let i = 0; i < this.options.iterations; i++ ) {
				this.gradientDescent()
				this.recordMSE()
				this.updateLearningRate()
			}
		}

		recordMSE() {
			const mse = this.features
							.matMul( this.weights )
							.sub( this.labels )
							.pow( 2 )
							.sum()
							.div( this.features.shape[ 0 ] )
							.get()

			this.mseHistory.unshift( mse )
		}

		updateLearningRate() {
			if ( this.mseHistory.length < 2 ) {
				return
			}

			if ( this.mseHistory[ 0 ] > this.mseHistory ) {
				this.options.learningRate /= 2
			} else {
				this.options.learningRate *= 1.05
			}
		}
	}

```



---



# plotting data with javascript

using the 'node remote plot' plugin

```js

	const plot = require( 'node-remote-plot' )

	plot( {
		x: regression.mseHistory.reverse(),
		xLabel: 'Iteration #',
		yLabel: 'Mean Squared Error'
	} );

```



---



# gradient descent alterations



## definitions

**why do it?**
decrease time needed to reach optimal mse for huge datasets
as we are running gradient descent more often rather than wait for whole datasets to be run

*gradient descent*
use entire feature set to update m & b

*batch gradient descent*
use a couple of observations at a time to update m & b

*stochastic gradient descent*
use one observation at a time to update m & b



## batch gradient descent

```js

	const regression = new LinearRegression( features, labels, {
		learningRate: .1,
		iterations: 100,
		//	adding batch size
		batchSize: 10
	} )

	class LinearRegression {
		...

		//	let features & labels to be passed in instead of this.

		//	change this.features to features
		gradientDescent( features, labels ) {
			const currentGuesses = features.matMul( this.weights )
			const differences = currentGuesses.sub( labels )

			const slopes = features
							.transpose()
							.matMul( differences )
							.div( features.shape[ 0 ] )

			this.weights = this.weights.sub( slopes.mul( this.options.learningRate ) )
		}

		train() {
			const batchQuantity = Math.floor(
				this.features.shape[ 0 ] / this.options.batchSize
			)

			for ( let i = 0; i < this.options.iterations; i++ ) {
				for ( let j = 0; j < batchQuantity; j++ ) {
					const { batchSize } this.options
					const startIndex = j * batchSize

					const featureSlice = this.features.slice(
						[ startIndex, 0 ],
						[ batchSize, -1 ]
					)

					const labelSlice = this.label.slice(
						[ startIndex, 0 ],
						[ batchSize, -1 ]
					)

					this.gradientDescent( featureSlice, labelSlice )
				}

				this.recordMSE()
				this.updateLearningRate()
			}
		}
	}

```



## stochastic gradient descent

```js

	const regression = new LinearRegression( features, labels, {
		learningRate: .1,
		iterations: 100,
		//	batch is single observation
		batchSize: 1
	} );

```



## making predictions

```js

	predict( observation ) {
		return this.processFeatures( observations ).matMul( this.weights )
	}

	regression.predict( [
		[ 120, 2, 380 ],
		[ 123, 1.8, 400 ]
	] ).print();

```



---



# natural binary classification



## types of regression

*linear regression*
predicts continuous values

*logistic regression*
predicts discrete values ( classification )

*binary classification*
binary options only
	yes or no
	spam or not spam



## sigmoid equation

if mx + b ( straight line slope ) will never give us an appropriate fit
we can use the sigmoid equation instead ( sigmoid looks like a sharper s )

1 / 1 + e to the power of -z
where e is euler's constant = 2.718

z = m * input + b



## decision boundaries

as sharp as the curve is, it is still a curve
so we need a decision boundary to cut off to a binary result
eg. anything lesser than .5 is 0, above is 1



## encoding label values

```js

	//	additional option in loadcsv
	converters: {
		passedemissions: value => {
			return value === 'TRUE' ? 1 : 0
		}
	}

```



## updating linear regression for logistic regression

simply run sigmoid on mx+b equations

```js

	class LogisticRegression {
		...

		gradientDescent( features, labels ) {
			//	add .sigmoid here
			const currentGuesses = features.matMul( this.weights ).sigmoid()
			const differences = currentGuesses.sub( labels )

			const slope = features
							.transpose()
							.mathMul( differences )
							.div( features.shape[ 0 ] )

			this.weights = this.weights.sub( slopes.mul( this.options.learningRate ) )
		}

		predict( observations ) {
			//	add .sigmoid here
			return this.processFeatures( observations )
						.matMul( this.weights )
						.sigmoid()
		}

		...
	}

```



## implementing a test function

```js

	predict( observations ) {
		return this.processFeatures( observations )
					.matMul( this.weights )
					.sigmoid()
					//	add greater( .5 ) = 50%, above -> 1, below -> 0
					.greater( this.options.decisionBoundary )
					//	treat results as numbers instead of a bool of 1/0
					.cast( 'float32' )
	}

	test( testFeatures, testLabels ) {
		const predictions = this.predict( testFeatures )

		testLabels = tf.tensor( testLabels )

		const incorrect = predictions
							.sub( testLabels )
							.abs()
							.sum()
							.get()

		return ( predictions.shape[ 0 ] - incorrect ) / predictions.shape[ 0 ]
	}

```



## mse history to cost history

```js

	recordCost() {
		const guesses = this.features.matMul( this.weights ).sigmoid()

		const termOne = this.labels
							.transpose()
							.matMul( guesses.log() )

		const termTwo = this.labels
							.mul( -1 )
							.add( 1 )
							.transpose()
							.matMul(
								guesses()
								.mul( -1 )
								.add( 1 ).
								log()
							)

		const cost = termOne.add( termTwo )
							.div( this.features.shape[ 0 ] )
							.mul( - 1 )
							.get( 0, 0 )

		this.costHistory.unshift( cost )
	}

	//	change all other spots with mseHistory to costHistory
	//	recordMSE to recordCost

```



---



# multi-value classification



## multinominal logistic regression

instead of 1 or 0 binary, have a list of possible results

```js

	//	change the init weights
	//	in constructor to be dynamic
	//	based on length aka shape[ 1 ] passed in
	this.weights = tf.( [ this.features.shape[ 1 ], this.labels.shape[ 1 ] ] )

```



## marginal vs conditional probability

sigmoid is marginal probability
softmax is conditional probability

*marginal probability distribution*
-	considers one possible output case in isolation
-	possible to have two or more same probability
-	if you add all probability together, can exceed 1

*conditional probability distribution*
-	considers all possible output cases together
-	cannot have same probability results
-	if you add all probability together, totals 1



## sigmoid to softmax

```js

	//	simply change all sigmoid() to softmax()
	//	ie.
	const guesses = this.features.matMul( this.weights ).sigmoid()
	//	to
	const guesses = this.features.matMul( this.weights ).softmax();

```



## implementing accuracy gauges

argMax will return column with highest value
ie.
[ 1, 0, 0 ] will return [ 0 ] as column 0 is the one with highest value of 3 columns

```js

	predict( observations ) {
		return this.processFeatures( observations )
					.matMul( this.weights )
					.softmax()
					.argMax( 1 )
	}

	test( testFeatures, testLabels ) {
		const predictions = this.predict( testFeatures )
		testLabels = tf.tensor( testLabels ).argMax( 1 )

		const incorrect = predictions
							.notEqual( testLabels )
							.sum()
							.get()

		return ( predictions.shape[ 0 ] - incorrect ) / predictions.shape[ 0 ]
	}

```



---



# image recognition in action



## flattening image data

```js

	//	npm library to handle mnist data
	const mnist = require( 'mnist-data' )

	//	gets one set out
	const mnistData = mnist.training( 0, 1 )

	//	flatmap will flatten the nested array to a single array
	const features = mnistData.images.values.map( image => _.flatMap( image ) );

```



## encoding label values

```js

	const encodedLabels = mnist.labels.values.map( label => {
		//	there are 10 possible results
		//	fill all with 0 first
		const row = new Array( 10 ).fill( 0 )
		//	switch the correct label to 1
		row[ label ] = 1
		return row
	} );

```



## implementing & accuracy gauge

```js

	const testMnistData = mnist.testing( 0, 1000 )
	const testFeatures = testMnistData.images.values.map( image => _.flatmap( image ) )
	const testEncodedLabels = testMnistData.labels.values.map( label => {
		const row = new Array( 10 ).fill( 0 )
		row[ label ] = 1
		return row
	} )

	const accuracy = regression.test( testFeatures, testEncodedLabels );

```



## dealing with zero variances

zeros are causing issues with standardization step we're doing

```js

	standardize( features ) {
		const { mean, variance } = tf.moments( features, 0 )

		//	change 0, 1 values to a bool, 0 becomes 1
		//	switch 1 to 0, vice versa
		//	change back to a number
		const filler = variance.cast( 'bool' ).logicalNot().cast( 'float32' )

		this.mean = mean
		//	where there's value of 0 in variance, it becomes a 1
		this.variance = variance.add( filler )

		return features.sub( mean ).div( this.variance.pow( .5 ) )
	}

```



---



# performance optimization



## the javascript garbage collector

once we are no longer able to access certain values
ie. scope closed with no reference to inner values
garbage collector will clear it
if you return results, etc so you can still reference things
garbage collector will leave it in memory



## shallow vs retained memory

*shallow*
actual memory usage

*retained*
the object has a reference to another item
this is retained memory



## releasing references

wrap the loading of data into a function & only pass back needed values
garbage collector will kick in & clear the rest

```js

	function loadData() {
		const mnistData = mnist.training( 0, 60000 )

		const features = mnist.images.map( image => _.flatmap( image ) )
		const encodedLabels = mnist.labels.values.map( label => {
			const row = new Array( 10 ).fill( 0 )
			row[ label ] = 1
			return row
		} )

		return { features, labels: encodedLabels }
	}

	const { features, labels } = loadData();

```



## implementing tf tidy

tensorflow creates a tensor for every calculation/chain

```js

	//	wrap any tensor creation with
	this.weights = tf.tidy( ()=> {

		const featureSlice = this.features.slice(
			[ startIndex, 0 ],
			[ batchSize, -1 ]
		)

		const labelSlice = this.labels.slice(
			[ startIndex, 0 ],
			[ batchSize, -1 ]
		)

		//	gradientDescent call also will have its tensors cleaned
		return this.gradientDescent( featureSlice, labelSlice )

	} );

```



## nan in cost history

```js

	//	add a very very small no.
	//	so we won't have 0 to be log()
	//	log 0 will becomes negative infinity
	const termTwo = this.labels
						.mul( - 1 )
						.add( 1 )
						.transpose()
						.matMul(
							guesses
							.mul( - 1 )
							.add( 1 )
							.add( 1e-7 ) // 1 x 10 ^ -1 = 0.00000001
							.log()
						);

```



## improving model accuracy

play around with

-	**learningRate**
-	**iterations**
-	**batchSize**



---



# custom csv loader



## reading files from disk

```js

	//	inbuilt module, don't have to npm install
	const fs = require( 'fs' )

	//	npm install lodash for helpers
	const _ = require( 'lodash' )

	function loadCSV( filename, options ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )
	}

```



## splitting into columns

```js

	const fs = require( 'fs' )
	const _ = require( 'lodash' )

	function loadCSV( filename, options ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )

		//	split data by line breaks
		//	in csv data is in rows with line breaks to next row
		//	first row is header row
		data = data.split( '\n' )
					//	each row in csv is then split to columns by a comma ','
					//	split them to make array of arrays
					.map( row => row.split( ',' ) )
	}

```



## dropping trailing columns

```js

	const fs = require( 'fs' )
	const _ = require( 'lodash' )

	function loadCSV( filename, options ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )

		data = data.split( '\n' )
					.map( row => row.split( ',' ) )

		//	most bad exports of csv will have columns of extra empty columns
		//	we use a lodash method to clean those out
		data = data.map( row => _.dropRightWhile( row, val => val === '' ) )
	}

```



## parsing number values

```js

	const fs = require( 'fs' )
	const _ = require( 'lodash' )

	function loadCSV( filename, options ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )

		data = data.split( '\n' )
					.map( row => row.split( ',' ) )

		data = data.map( row => _.dropRightWhile( row, val => val === '' ) )

		//	get first row, it's the headers!
		const headers = _.first( data )

		data = data.map( ( row, index ) => {
			//	if it's first row, it's headers
			//	don't parse it
			if ( index === 0 ) {
				return row
			}

			row.map( ( element, index ) => {
				//	data in csv comes in as strings
				//	parseFloat makes strings into numbers
				const result = parseFloat( element )
				//	if result is not number return string, else return number
				return _.isNan( result ) ? element : result
			} )
		} )
	}

```



## custom value parsing

```js

	const fs = require( 'fs' )
	const _ = require( 'lodash' )

	//	have a converter option which is by default an empty object
	//	if you need a custom converter function
	//	pass the function in as an option after filename
	function loadCSV( filename, { converters = {} } ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )

		data = data.split( '\n' )
					.map( row => row.split( ',' ) )

		data = data.map( row => _.dropRightWhile( row, val => val === '' ) )

		const headers = _.first( data )

		data = data.map( ( row, index ) => {
			if ( index === 0 ) {
				return row
			}

			row.map( ( element, index ) => {
				//	if there is converters
				if ( converters[ headers [ index ] ] ) {
					const converted = converters[ headers[ index ] ]( element )
					return _.isNan( converted ) ? element : converted
				}

				const result = parseFloat( element )
				return _.isNan( result ) ? element : result
			} )
		} )
	}

	//	add examples of passing in options
	//	add a converter option in to make string 'TRUE' to 1 or 0
	loadCSV( 'data.csv', {
		converters: {
			passed: val => ( val === 'TRUE' ? 1 : 0 )
		}
	} );

```



## extracting data columns

```js

	const fs = require( 'fs' )
	const _ = require( 'lodash' )

	//	add extract columns function to do extraction
	//	pass in data & name of columns to pull
	function extractColumns( data, columnNames ) {
		//	get headers out
		const headers = _.first( data )
		//	find index of column names in headers
		const indexes = _.map( columnNames, column => headers.indexOf( column ) )
		//	put out data at the index at every row
		const extracted = _.map( data, row => _.pullAt( row, indexes ) )

		return extracted
	}

	//	add two more options into loadCSV, these are empty arrays
	function loadCSV( filename, { converters = {}, dataColumns = [], labelColumns = [] } ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )

		data = data.split( '\n' )
					.map( row => row.split( ',' ) )

		data = data.map( row => _.dropRightWhile( row, val => val === '' ) )

		const headers = _.first( data )

		data = data.map( ( row, index ) => {
			if ( index === 0 ) {
				return row
			}

			row.map( ( element, index ) => {
				if ( converters[ headers [ index ] ] ) {
					const converted = converters[ headers[ index ] ]( element )
					return _.isNan( converted ) ? element : converted
				}

				const result = parseFloat( element )
				return _.isNan( result ) ? element : result
			} )
		} )

		//	run the extractColumns with the info passed in from options
		let labels = extractColumns( data, labelColumns )
		data = extractColumns( data, dataColumns )

		//	dump the headers as we no longer need them
		//	only needed to get the indexes to use in extractColumns
		data.shift()
		labels.shift()
	}

	//	add in extraction options of arrays
	loadCSV( 'data.csv', {
		dataColumns: [ 'height', 'value' ],
		labelColumns: [ 'passed' ],
		converters: {
			passed: val => ( val === 'TRUE' ? 1 : 0 )
		}
	} );

```



## shuffling data via seed phrase

```js

	const fs = require( 'fs' )
	const _ = require( 'lodash' )

	//	npm install shuffle-seed
	//	library to help with shuffling data
	const shuffleSeed = require( 'shuffle-seed' )

	function extractColumns( data, columnNames ) {
		const headers = _.first( data )
		const indexes = _.map( columnNames, column => headers.indexOf( column ) )
		const extracted = _.map( data, row => _.pullAt( row, indexes ) )

		return extracted
	}

	//	add shuffle option with default to true
	function loadCSV( filename, {	converters = {},
									dataColumns = [],
									labelColumns = [],
									shuffle = true } ) {
		let data = fs.readFileSync( filename, { 'utf-8' } )

		data = data.split( '\n' )
					.map( row => row.split( ',' ) )

		data = data.map( row => _.dropRightWhile( row, val => val === '' ) )

		const headers = _.first( data )

		data = data.map( ( row, index ) => {
			if ( index === 0 ) {
				return row
			}

			row.map( ( element, index ) => {
				if ( converters[ headers [ index ] ] ) {
					const converted = converters[ headers[ index ] ]( element )
					return _.isNan( converted ) ? element : converted
				}

				const result = parseFloat( element )
				return _.isNan( result ) ? element : result
			} )
		} )

		let labels = extractColumns( data, labelColumns )
		data = extractColumns( data, dataColumns )

		data.shift()
		labels.shift()

		if ( shuffle ) {
			//	'phrase' can be any string
			//	but passing the same string to your different shuffles
			//	will result in the same shuffle
			//	aka the phrase is a algo step to shuffle
			//	same phrase is same steps, thus same mixture
			data = shuffleSeed.shuffle( data, 'phrase' )
			labels = shuffleSeed.shuffle( data, 'phrase' )
			//	you can pass in a phrase with shuffle too if needed
			//	so you can customise your shuffles by passing in an additional option
		}
	}

	loadCSV( 'data.csv', {
		dataColumns: [ 'height', 'value' ],
		labelColumns: [ 'passed' ],
		shuffle: true,
		converters: {
			passed: val => ( val === 'TRUE' ? 1 : 0 )
		}
	} );

```



---
