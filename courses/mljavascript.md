


<https://www.udemy.com/machine-learning-with-javascript/>



# what is machine learning



## problem solving process

1.	identify the independent & dependent variables
	*features* - are categories of data points that affect the value of a label

2.	assemble a set of data related to the problem you're trying to solve
	datasets almost always require cleanup or formatting

3.	decide on the type of output you are predicting
	*classification* - labels belong to a discret set
	*regression* - labels belong to a continuous set

4.	based on type of output,
	pick an algorithm that will determine a correlation between your 'feature' & 'labels'
	many algorithms exist, each with pros & cons

5.	use model generated by algorithm to make a prediction
	models relate the value of 'features' to the value of 'labels'



---



# algorithm overview



## k-nearest neighbor ( knn )

one of the simplest machine learning algorithms
essentially averaging the most similar inputs previously & their results

*eg. which bucket will a ball go into if dropped at 300px*

-	drop a ball a bunch of times all around the board
-	for each observation, subtract drop point from 300px to get absolute value
-	sort the results from least to most
-	look at the 'k' top records, what was the most frequent bucket
-	the more frequent bucket is our prediction

*note: k is a generic number which can be experimented & adjusted to suit different tasks*



## lodash

javascript utility library

```js

	const numbers = [
		[ 10, 5 ],
		[ 17, 2 ],
		[ 34, 1 ]
	]

	const sorted = _.sortBy( numbers, row => row[ 1 ] )

	//	results in
	sorted = [
		[ 34, 1 ],
		[ 17, 2 ],
		[ 10, 5 ]
	]

	const mapped = _.map( sorted, row => row[ 1 ] )

	//	results in
	mapped = [ 1, 2, 5 ]

	//	chaining
	//	_.chain to start chain
	//	.value() to get result
	const chained = _.chain( numbers )
						.sortBy( row => row[ 1 ] )
						.map( row => row[ 1 ] )
						.value();

```



## implementing knn

```js

	const outputs = [
		//	drop position, bounciness, ball size, result
		[ 10, .5, 16, 1 ],
		[ 200, .5, 16, 4 ],
		[ 350, .5, 16, 4 ],
		[ 600, .5, 16, 5 ]
	]

	const
	predictionPoint = 300,
	k = 3

	function distance( point, predictionPoint ) {
		return Math.abs( point - predictionPoint )
	}

	//	using lodash
	_.chain( outputs )
		//	get abs difference & results into an array, ignoring bounce & size
		.map( row => [ distance( row[ 0 ], predictionPoint ), row[ 3 ] ] )
		//	sort difference from lowest to highest
		.sortBy( row => row[ 0 ] )
		//	get top k records
		.slice( 0, k )
		//	get object with value of row[ 1 ] & number of times it occurs
		//	countBy changes value/keys into strings
		//	note to change back to num later
		.countBy( row => row[ 1 ] )
		//	take object & convert to array of arrays
		.toPairs( )
		//	sort most commonly occurring bucket from low to high
		.sortBy( row => row[ 1 ] )
		//	return last element aka [ "4", "2" ]
		.last()
		//	return first element aka "4"
		.first()
		//	change to num
		.parseInt()
		.value();

```



## training & test data

split all data into training & test

*training&* is used to implement the knn formula, aka train our algo
*test* use the trained algo & check how accurate our algo is

note: always shuffle data before splitting to ensure no patterns exists in our data
eg. our data could come in from small to large or arranged in some particular order we don't know about

```js

	function splitDataset( data, testCount ) {
		const shuffled = _.shuffle( data )

		const testSet = _.slice( shuffled, 0, testCount )
		const trainingSet = _.slice( shuffled, testCount )

		return [ testSet, trainingSet ]
	}

	//	modified from above to be generalized
	function knn( data, point ) {
		return _.chain( data )
			.map( row => [ distance( row[ 0 ], point ), row[ 3 ] ] )
			.sortBy( row => row[ 0 ] )
			.slice( 0, k )
			.countBy( row => row[ 1 ] )
			.toPairs()
			.sortBy( row => row[ 1 ] )
			.last()
			.first()
			.parseInt()
			.value()
	}

	function runAnalysis() {
		const testSetSize = 10
		const [ testSet, trainingSet ] = splitDataset( outputs, testSetSize )

		let numberCorrect = 0

		for ( let i = 0; i < testSet.length; i++ ) {
			const bucket = knn( trainingSet, testSet[ i ][ 0 ] )

			if ( bucket === test[ i ][ 3 ] ) {
				numberCorrect++
			}
		}

		console.log( 'Accuracy', numberCorrect / testSetSize )
	}

```



## refactoring accuracy report

```js

	//	refactored from top
	function runAnalysis() {
		const testSetSize = 10
		const [ testSet, trainingSet ] = splitDataset( outputs, testSetSize )

		const accuracy = _.chain( testSet )
			.fliter( testPoint => knn( trainingSet, testPoint[ 0 ] ) === testPoint[ 3 ] )
			.size()
			.divide( testSetSize )
			.value()

		console.log( 'Accuracy is ', accuracy )
	}

```



## investigating optimal k values

```js

	function runAnalysis() {
		const testSetSize = 50
		const [ testSet, trainingSet ] = splitDataset( outputs, testSetSize )

		_.range( 1, 15 ).forEach( k => {
			const accuracy = _.chain( testSet )
				.fliter( testPoint => knn( trainingSet, testPoint[ 0 ], k ) === testPoint[ 3 ] )
				.size()
				.divide( testSetSize )
				.value()

			console.log( 'For k of ', k, ' accuracy is ', accuracy )
		}
	}

```



## multi-dimensional knn

updating the distance function to use the pythagorean theorem

`c ** 2 = a ** 2 + b ** 2`
`c = ( a ** 2 + b ** 2 ) ** .5`

with more than 2 features, we just add more x ** 2 into equation

```js

	function distance( pointA, pointB ) {

		//	where pointA is an array
		_.chain( pointA )
			//	zip takes arrays
			//	makes each pair at point into its own array
			.zip( pointB )
			.map( ( [ a, b ] ) => ( a - b ) ** 2 )
			.sum()
			.value() ** .5
	}

	//	returns array without the last data
	//	without modifying actual array
	_.initial( row )

	//	returns last data of array
	//	without modifying actual array
	_.last( row);

```



## magnitude offsets in features

because the range of different features may be very different
the effects of it on the calculation might become offscaled
thus we need to normalize our data or standardize them

-	*normalize*
	make data range between 0 to 1
	data will only be between 0 to 1

-	*standardize*
	make majority of data near 0, from -1 to 1
	possible for outliers to exceed -1 or 1

```js

	//	get min & max value
	//	normalize points to minmax

	const points = [ 200, 150, 650, 430 ]

	const min = _.min( points )
	const max = _.max( points )

	_.map( points, point => {
		return ( point - min ) / ( max - min )
	} );

```



## normalization with minmax

```js

	//	feature count will let us know
	//	that the last data in array is its result
	//	not a feature
	function minMax( data, featureCount ) {
		//	clone so we don't modify actual data
		const clonedData = _.cloneDeep( data )

		//	i is column in clonedData array
		//	getting each feature we want to normalize
		for ( let i = 0; i < featureCount; i++ ) {
			const column = clonedData.map( row => row[ i ] )

			const min = _.min( column )
			const max = _.max( column )

			//	j is row in clonedData array
			//	getting each set of data
			for ( let j = 0; j <  clonedData.length; j++ ) {
				//	modify array
				clonedData[ j ][ i ] = ( clonedData[ j ][ i ] - min ) / ( max - min )
			}
		}

		return clonedData
	}

```



---



# onwards to tensorflow js



---



# applications of tensorflow



---



# getting started with gradient descent



---



# gradient descent with tensorflow



---



# increasing performance with vectorized solutions



---



# plotting data with javascript



---



# gradient descent alterations



---



# natural binary classification



---
